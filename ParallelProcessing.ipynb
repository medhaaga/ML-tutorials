{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT 548/CSE 547 Tutorial: Parallel Processing\n",
    "\n",
    "### **Instructor**:</b> Marina Meila\n",
    "### **TA**: Medha Agarwal\n",
    "### **Date**: May 09, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è∞ Previously...\n",
    "\n",
    "We looked at vectorization - both as a process of converting scalar operations (loop-based) into array operations and as a method for converting input data from its raw format (i.e. text ) into vectors of real numbers.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò Tutorial Overview\n",
    "\n",
    "In this tutorial, we will look at methods for paralell processing using multiple cores of CPU and GPUs. We iwll look at applications like vectorization, embarassingly parallel loops, or ML model training that benefit from parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Compute Environments\n",
    "\n",
    "\n",
    "### 1.1. Basic Hardware\n",
    "\n",
    "*Source:* [NVIDIA](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/) and [TDS](https://towardsdatascience.com/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d)\n",
    "\n",
    "- Processors are electronic circuitry that execute computer instructions, whose minimal unit is a **transistor**. \n",
    "- The two standard processors on most systems are called the **Central Processing Unit (CPU)** and the **Graphic Processing Unit (GPU)**.\n",
    "- These processors differ in their allotment of transistors to computing processes. \n",
    "    - CPUs dedicate these transistors into a small number of **logical cores**, with a stronger emphasis on circuitry used for **caches** (data storage that speeds up access) and **control flow** (`if-else` statements, `for` and `while` loops, etc). \n",
    "    - GPUs, on the other hand, can have thousands of cores, each capable only of simple arithmetic operations.\n",
    "- Thus, for highly parallelizable operations (those that can be split into independent **threads**), such as matrix multiplications common in ML/DL, GPUs have become an essential part of any computing infrastructure.\n",
    "- That being said, parallelizing more complex operations, such as hyperparameter search, is still done on machines with a large number of CPU cores. Examples include **CPU Clusters** such as the Stat cluster, which uses a manager called Slurm to schedule computation.\n",
    "- **Tensor Processing Units (TPUs)** are processors that are built specifically for the tensor operations in neural networks training. Specifically, it was made to speed up the operations of TensorFlow, which used to be the dominant DL framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `htop` in a terminal in order to \"see\" the physical CPU cores at work. When running operations that make use of all of the cores (such as a large matrix multiplications), you should see the bars light up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graceful Exit\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "n_sim = 10000\n",
    "n = 10000\n",
    "\n",
    "try:\n",
    "    for i in range(10000):\n",
    "        A = np.random.normal(size=(n, n))\n",
    "        B = np.random.normal(size=(n, n))\n",
    "\n",
    "        C = A @ B\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Graceful Exit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working on a server, use `tmux` to let processes run in the background while you can break the connection. This is not only a convenient method, but very safe as well. A few relevant commands:\n",
    "- `tmux`: Create a session.\n",
    "- `ctrl + B`, then `%`: Create a panel within a session.\n",
    "- `ctrl + B`, then `D`: Detach from a session.\n",
    "- `tmux ls`: List the current sessions.\n",
    "- `tmux attach -t <session_name>`: Attach to a running session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parallelism over CPU and GPU\n",
    "\n",
    "Parallel programming is a method of speeding up computations by dividing tasks across multiple processing units that execute simultaneously. In machine learning, this is essential for handling large datasets and complex models efficiently. \n",
    "\n",
    "- CPU design philosophy: optimized for sequential processing\n",
    "\n",
    "  - Fewer, more powerful cores with complex control units\n",
    "  - Large caches and branch prediction\n",
    "  - Optimized for irregular memory access and control flow\n",
    "  - Parallelism achieved through multithreading or multiprocessing - where a few powerful cores execute different parts of a task concurrently‚Äîuseful for data loading or feature processing.\n",
    "\n",
    "- GPU design philosophy: optimized for parallel processing\n",
    "  - Thousands of simpler cores\n",
    "  - Specialized for floating-point operations\n",
    "  - SIMT (Single Instruction, Multiple Thread) execution model\n",
    "  - Memory architecture optimized for throughput\n",
    "  - well-suited for operations like matrix multiplication and deep learning training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Application 1: Vectorized Implementation\n",
    "\n",
    "**Implicit Parallelism**\n",
    "\n",
    "- Because matrix operations are the prototypical use case of parallelism, many computing processes can benefit from parallelization implicitly by being represented in linear algebraic form.\n",
    "- Such implementations are said to be **vectorized**. The main idea is to avoid all `for` loops where possible, and use libraries like `numpy` and `pytorch` for matrix and tensor algebra.\n",
    "- To develop software/algorithms in any kind of large-scale or production setting, it is very important to make vectorization a constant habit. A core part of why GPUs (and even CPUs) make processes faster is by distributing the load across all of the logical cores. Using `for` loops ruins this benefit and wastes valuable compute time.\n",
    "- Another separate but related issue is list concatenation; if you are trying to apply a function to a sequence of outputs, it is sometimes tempting to have a \"running\" list for which you concatenate the new value through the iteration of a `for` loop. In these sitatuations, it is always better to **pre-allocate** arrays, or be confident that you are **appending** (in $O(1)$) instead of copying the entire contents of the array every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Nadaraya-Watson with Gaussian Kernel\n",
    "\n",
    "In this example, we implement multiple versions of the Nadaraya-Watson regression estimator. Specifically, consider a supervised learning problem in which we are given an $n$-sized training set $(x_1, y_1), ..., (x_n, y_n)$ of feature vectors $x_i \\in \\mathbb{R}^d$ and label $y_i \\in \\mathbb{R}$. Given a test point $x \\in \\mathbb{R}^d$, the predicted value of the label $\\hat{y}$ is given by\n",
    "\\begin{equation}\n",
    "    \\hat{y} = \\sum_{i=1}^n \\frac{y_i k\\left(\\frac{x - x_i}{h}\\right)}{\\sum_{j=1}^n k\\left(\\frac{x - x_j}{h}\\right)},\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{align*}\n",
    "    k(z) = e^{-\\frac{1}{2}||z||^2}\n",
    "\\end{align*}\n",
    "is the Gaussian kernel and and $h > 0$ is a bandwidth parameter. Kernel methods offer plenty of opportunities to replace wasteful `for` loops with efficient vectorized implementations, especially when $n$ and $d$ are very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_gaussian_kernel(x1, x2, h):\n",
    "    sq_norm = 0\n",
    "    for j in range(len(x1)):\n",
    "        sq_norm += (x1[j] - x2[j]) ** 2\n",
    "    return np.exp(-0.5 * sq_norm / h ** 2)\n",
    "\n",
    "class NaiveNadarayaWatsonRegressor:\n",
    "    def __init__(self, bandwidth, kernel=naive_gaussian_kernel):\n",
    "        self.h = bandwidth\n",
    "        self.kernel = kernel\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = check_array(X)\n",
    "        \n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            num = 0\n",
    "            denom = 0\n",
    "            for i, x_i in enumerate(self.X_train):\n",
    "                num += self.y_train[i] * self.kernel(x, x_i, self.h)\n",
    "                denom += self.kernel(x, x_i, self.h)\n",
    "            y_pred.append(num / denom)\n",
    "            \n",
    "        return np.array(y_pred).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Why does the `fit` method above simply store the training data? \n",
    "\n",
    "**Exercise 2:** What are other algorithms that might be implemented in this way? (These are referred to as **lazy learners**.\n",
    "\n",
    "**Exercise 3:** Based on our discussion of nearest neighbors in high dimensions, what kinds of operations could occur in the `fit` method for approximate lazy learners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, d):\n",
    "    X = np.random.normal(size=(n, d))\n",
    "    coef = np.random.normal(size=(d,))\n",
    "    intercept = np.random.normal()\n",
    "    noise = np.random.normal(size=(n,))\n",
    "    y = (X @ coef + intercept + noise).reshape(-1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a test set $(x_1, y_1), ..., (x_n, y_n)$, we measure performance using the $R^2$ score (also known as explained variance).\n",
    "\\begin{equation}\n",
    "    R^2 = \\frac{\\sum_{i=1}^n (\\hat{y}_i - y_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2},\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "    \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "\\end{equation}\n",
    "is the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 10)\n",
      "(8000,)\n",
      "(2000, 10)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "# DEMO: \n",
    "np.random.seed(123)\n",
    "\n",
    "n = 10000\n",
    "d = 10\n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_data(n, d)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too slow! :'(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    start = time.time()\n",
    "    naive_model = NaiveNadarayaWatsonRegressor(bandwidth=0.5).fit(X_train, y_train)\n",
    "    y_pred = naive_model.predict(X_test)\n",
    "    print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "    end = time.time()\n",
    "    print(\"Time: %0.4f seconds.\" % (end - start))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Too slow! :'(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give evalutation set $z_1, ..., z_m$, let $K \\in \\mathbb{R}^{m \\times n}$ be the matrix such that \n",
    "\\begin{align*}\n",
    "    K_{ij} = k\\left(\\frac{z_i - x_j}{h}\\right).\n",
    "\\end{align*}\n",
    "Then, let $y = (y_1, ..., y_n)^\\top \\in \\mathbb{R}^n$ and $\\hat{y} = (\\hat{y}_1, ..., \\hat{y}_m)^\\top \\in \\mathbb{R}^m$. It holds that \n",
    "\\begin{align*}\n",
    "\\hat{y} = \\frac{Ky}{K 1_n},\n",
    "\\end{align*}\n",
    "where the division is element-wise. This will lead to a vectorized implementation. For now, we use the `sklearn` function for `pairwise_distances`, but we will soon see how to make a vectorized implementation of this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedNadarayaWatsonRegressor:\n",
    "    def __init__(self, bandwidth):\n",
    "        self.h = bandwidth\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = check_array(X)\n",
    "        \n",
    "        # n_eval by n_train kernel matrix.\n",
    "        kernels = np.exp(-0.5 * pairwise_distances(X, self.X_train) ** 2 / self.h ** 2)\n",
    "        \n",
    "        # Element-wise division\n",
    "        return (kernels @ self.y_train) / kernels.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** How can we use matrix operations to implement a vectorized version of pairwise Euclidean distances? That is, given the matrix $X \\in \\mathbb{R}^{n_1 \\times d}$, and the matrix $Z \\in \\mathbb{R}^{n_1 \\times d}$, how can we produce the matrix $D \\in \\mathbb{R}^{n_1 \\times n_2}$, where\n",
    "\\begin{align*}\n",
    "    D_{ij} = ||x_i - z_j||_2 ?\n",
    "\\end{align*}\n",
    "\n",
    "**Solution:** Represent the Euclidean distance as a scalar product.\n",
    "\\begin{align*}\n",
    "    D_{ij} = ||x_i - z_j||_2 = \\sqrt{(x_i - z_j)^\\top (x_i - z_j)} = \\sqrt{x_i^\\top x_i + z_j^\\top z_j - 2 x_i^\\top z_j}.\n",
    "\\end{align*}\n",
    "The $x_i^\\top x_i$ and $z_j^\\top z_j$ terms can be computed by taking the norms of the rows of $X$ and $Z$. The cross term $x_i^\\top z_j$ can be computed via the matrix multiplication $X^\\top Z$. Do we need to compute $X^\\top X$ or $Z^\\top Z$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.7880181382483236\n",
      "Time: 0.3733 seconds.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    start = time.time()\n",
    "    vectorized_model = VectorizedNadarayaWatsonRegressor(bandwidth=0.5).fit(X_train, y_train)\n",
    "    y_pred = vectorized_model.predict(X_test)\n",
    "    print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "    end = time.time()\n",
    "    print(\"Time: %0.4f seconds.\" % (end - start))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Too slow! :'(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending to the GPU\n",
    "\n",
    "- These kinds of operations can be made even faster by using a GPU. In order to interact with it, we use the `torch` package (PyTorch).\n",
    "- Using PyTorch is largely the same as `numpy`. The main difference is that the quintessential array object becomes a `torch.tensor` instead of a `numpy.ndarray`. Second, tensors exist on a \"device\" which can either be `\"cpu\"` for the CPU, or `\"cuda\"` for the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def set_device():\n",
    "    # If there's a GPU available...\n",
    "    if torch.cuda.is_available():\n",
    "\n",
    "        # Tell PyTorch to use the GPU.\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "        print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    # If not...\n",
    "    else:\n",
    "        print(\"No GPU available, using the CPU instead.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "def pdist(sample_1, sample_2, eps=1e-5):\n",
    "    n_1, n_2 = sample_1.size(0), sample_2.size(0)\n",
    "    norms_1 = torch.sum(sample_1**2, dim=1, keepdim=True)\n",
    "    norms_2 = torch.sum(sample_2**2, dim=1, keepdim=True)\n",
    "    norms = (norms_1.expand(n_1, n_2) +\n",
    "             norms_2.transpose(0, 1).expand(n_1, n_2))\n",
    "    distances_squared = norms - 2 * sample_1.mm(sample_2.t())\n",
    "    return torch.sqrt(eps + torch.abs(distances_squared))\n",
    "\n",
    "class GPUNadarayaWatsonRegressor:\n",
    "    def __init__(self, bandwidth):\n",
    "        self.h = bandwidth\n",
    "        self.device = set_device()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        self.X_train = torch.from_numpy(X).to(self.device)\n",
    "        self.y_train = torch.from_numpy(y).to(self.device)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = check_array(X)\n",
    "        X = torch.from_numpy(X).to(self.device)\n",
    "        \n",
    "        # n_eval by n_train kernel matrix.\n",
    "        kernels = torch.exp(-0.5 * pdist(X, self.X_train) ** 2 / self.h ** 2)\n",
    "        \n",
    "        # Element-wise division\n",
    "        return (torch.matmul(kernels, self.y_train) / kernels.sum(axis=1)).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: NVIDIA TITAN Xp\n",
      "R^2:  0.7880181382483237\n",
      "Time: 0.0287 seconds.\n"
     ]
    }
   ],
   "source": [
    "# DEMO: Run in Colab environment.\n",
    "try:\n",
    "    start = time.time()\n",
    "    gpu_model = GPUNadarayaWatsonRegressor(bandwidth=0.5).fit(X_train, y_train)\n",
    "    y_pred = gpu_model.predict(X_test)\n",
    "    print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "    end = time.time()\n",
    "    print(\"Time: %0.4f seconds.\" % (end - start))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Too slow! :'(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Application 2: Embarassingly Parallel Loops\n",
    "\n",
    "**Definition**\n",
    "\n",
    "- In the previous section we were using a combination of linear algebra representations and built-in functions to benefit from parallelization. This method only works for inherently mathematical objects, which is also the reason we got a speed up from the GPU.\n",
    "- When we would like to parallelize more complicated programming logic, we have to use explicit parallelization. This means we describe explicitly a \"batch\" operation which should be split among various threads.\n",
    "- A special (but very common) case of this situation is **embarrassingly parallel loops**. This occurs when there is a `for` loop for which the result of computation in each iteration has no relationship to the result of computation in other iterations. \n",
    "- In these cases, the code essentially \"looks\" like a `for` loop. We just have to instruct the machine to split this load over a certain number of cores.\n",
    "- One thing to note that explicit parallelism often has an overhead cost, so sometimes for small scale problems it can even be slower than just iterating sequentially. Doing this correctly requires some knowledge of the hardware system its running on.\n",
    "\n",
    "**Exercise 4.1:** Please name some iterated computations in machine learning settings that are and are not embarrassingly parallelizable. \n",
    "\n",
    "**Exercise 4.2:** Based on their description, are embarrassingly parallel workloads better distributed over CPU cores or GPU cores? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Exercise 4.1**\n",
    "\n",
    "*Embarrassingly parallelizable computations:*\n",
    "\n",
    "* **Inference on independent samples**: Making predictions on multiple test inputs where each input can be evaluated separately.\n",
    "* **Hyperparameter tuning** (e.g., grid search or random search): Each model configuration can be trained independently.\n",
    "* **Cross-validation folds**: Training models on different folds does not depend on the others.\n",
    "* **Data augmentation (offline)**: Applying random transformations to different images or sequences independently.\n",
    "\n",
    "*Not embarrassingly parallelizable computations:*\n",
    "\n",
    "* **Gradient descent with shared parameters**: Each iteration depends on the model state updated in previous steps.\n",
    "* **Sequential RNN training**: Later steps depend on hidden states from earlier ones.\n",
    "* **Backpropagation through time (BPTT)** in recurrent models: Requires passing gradients backward through dependent steps.\n",
    "* **Batch normalization during training**: Requires computing statistics (mean/variance) over the entire batch, introducing dependencies.\n",
    "\n",
    "**Exercise 4.2**\n",
    "\n",
    "*Embarrassingly parallel workloads are often better distributed over GPU cores**‚Äî**but it depends on the nature of the task*.\n",
    "\n",
    "* **GPUs** are ideal when the computation per task is numeric-heavy, uniform, and can be written in a vectorized or SIMD fashion. They offer massive throughput for simple operations on large datasets (e.g., image processing, vector math, matrix ops).\n",
    "* **CPUs** are often better when each task involves complex logic, branching, or irregular memory access, which GPUs handle less efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to implementation, typical options in Python are `joblib` and `multiprocessing` for parallel CPU processing and PyTorch with CUDA for GPU-based inference and augmentation. In R, the `apply` family is a common option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Inference on independent samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CPU parallelism (e.g., with scikit-learn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST (~70,000 samples, 784 features)\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Reduce size for faster training, but keep test large for benchmarking\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:20000], y[:20000], test_size=0.5, random_state=42)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000, solver='lbfgs', n_jobs=-1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial inference time: 1.1348 seconds\n",
      "CPU Parallel inference time: 0.6426 seconds\n"
     ]
    }
   ],
   "source": [
    "start_serial = time.time()\n",
    "\n",
    "y_pred_serial = [model.predict(x.reshape(1, -1))[0] for x in X_test]\n",
    "\n",
    "end_serial = time.time()\n",
    "print(f\"Serial inference time: {end_serial - start_serial:.4f} seconds\")\n",
    "\n",
    "def predict_one(x):\n",
    "    return model.predict(x.reshape(1, -1))[0]\n",
    "\n",
    "start_parallel = time.time()\n",
    "\n",
    "y_pred_parallel = Parallel(n_jobs=-1)(delayed(predict_one)(x) for x in X_test)\n",
    "\n",
    "end_parallel = time.time()\n",
    "print(f\"CPU Parallel inference time: {end_parallel - start_parallel:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using a GPU-compatible model (e.g., PyTorch), batch inference uses GPU cores naturally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n",
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: NVIDIA TITAN Xp\n",
      "GPU inference time: 1.1141 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST and move to GPU\n",
    "transform = transforms.ToTensor()\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "print(test_dataset.data.shape)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "# Simple model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize and move model to GPU\n",
    "device = set_device()\n",
    "model = SimpleNN().to(device)\n",
    "\n",
    "# Dummy training step (skipped here) ‚Äì just use random weights\n",
    "model.eval()\n",
    "\n",
    "# Inference on GPU\n",
    "start_gpu = time.time()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.append(preds.cpu())\n",
    "\n",
    "end_gpu = time.time()\n",
    "print(f\"GPU inference time: {end_gpu - start_gpu:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hyperparameter tuning\n",
    "\n",
    "We use `joblib` to search for the best bandwidth for the Nadaraya-Watson estimator. As before, use `htop` to ensure that your code is working properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidths = np.logspace(-1, 3, 30)\n",
    "n = 10000\n",
    "d = 10\n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_data(n, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential hyperparameter search takes 8.27 seconds.\n",
      "Parallelized hyperparameter search takes 2.44 seconds.\n"
     ]
    }
   ],
   "source": [
    "is_parallel = True\n",
    "\n",
    "# Define function that performs each iteration.\n",
    "def worker(bandwidth):\n",
    "    model = VectorizedNadarayaWatsonRegressor(bandwidth=bandwidth).fit(X_train, y_train)\n",
    "    return r2_score(y_test, model.predict(X_test))\n",
    "\n",
    "def search(is_parallel):\n",
    "    try:\n",
    "        if is_parallel:\n",
    "            # Use n_jobs to determine how many cores will be used. Negative values imply \"all but\".\n",
    "            r2 = Parallel(n_jobs=-2)(delayed(worker)(bandwidth) for bandwidth in bandwidths)\n",
    "        else:\n",
    "            r2 = []\n",
    "            for bandwidth in bandwidths:\n",
    "                r2.append(worker(bandwidth))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Too slow! :'('\")\n",
    "\n",
    "    return r2\n",
    "    \n",
    "start = time.time()\n",
    "r2 = search(False)\n",
    "print(f\"Sequential hyperparameter search takes {time.time() - start:.2f} seconds.\")\n",
    "\n",
    "start = time.time()\n",
    "r2 = search(True)\n",
    "print(f\"Parallelized hyperparameter search takes {time.time() - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(bandwidths, r2, linewidth=3, color=\"red\")\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Bandwidth\")\n",
    "plt.ylabel(r\"$R^2$ Score\")\n",
    "plt.title(\"Nadaraya-Watson Performance vs. Bandwidth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Cross-validation folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST...\n",
      "\n",
      "Running serial cross-validation...\n",
      "Serial CV Accuracy: 0.8660\n",
      "Serial Time: 46.67 seconds\n",
      "\n",
      "Running parallel cross-validation...\n",
      "Parallel CV Accuracy: 0.8660\n",
      "Parallel Time: 1.80 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ------------------------\n",
    "# Load and prepare dataset\n",
    "# ------------------------\n",
    "print(\"Loading MNIST...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist.data[:2000], mnist.target[:2000].astype(int)  # Subset for speed\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ------------------------\n",
    "# Serial Cross-Validation\n",
    "# ------------------------\n",
    "print(\"\\nRunning serial cross-validation...\")\n",
    "start_serial = time.time()\n",
    "serial_accuracies = []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    serial_accuracies.append(acc)\n",
    "\n",
    "end_serial = time.time()\n",
    "print(f\"Serial CV Accuracy: {np.mean(serial_accuracies):.4f}\")\n",
    "print(f\"Serial Time: {end_serial - start_serial:.2f} seconds\")\n",
    "\n",
    "# ------------------------\n",
    "# Parallel Cross-Validation\n",
    "# ------------------------\n",
    "print(\"\\nRunning parallel cross-validation...\")\n",
    "\n",
    "def worker(train_idx, val_idx):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    return accuracy_score(y_val, y_pred)\n",
    "\n",
    "start_parallel = time.time()\n",
    "\n",
    "parallel_accuracies = Parallel(n_jobs=-1)(\n",
    "    delayed(worker)(train_idx, val_idx) for train_idx, val_idx in kf.split(X)\n",
    ")\n",
    "\n",
    "end_parallel = time.time()\n",
    "print(f\"Parallel CV Accuracy: {np.mean(parallel_accuracies):.4f}\")\n",
    "print(f\"Parallel Time: {end_parallel - start_parallel:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modern multi-GPU Parallelism\n",
    "\n",
    "[*Source: Huggingface*](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many)\n",
    "\n",
    "Multi-GPU setups are effective for accelerating training and fitting large models in memory that otherwise wouldn‚Äôt fit on a single GPU. It relies on parallelizing the workload across GPUs. There are several types of parallelism such as data parallelism, tensor parallelism, pipeline parallelism, and model parallelism. Each type of parallelism splits the workload differently, whether it‚Äôs the data or the model.\n",
    "\n",
    "#### 1. Data parallelism\n",
    "\n",
    "Data parallelism evenly distributes data across multiple GPUs. Each GPU holds a copy of the model and concurrently processes their portion of the data. At the end, the results from each GPU are synchronized and combined. Data parallelism significantly reduces training time by processing data in parallel, and it is scalable to the number of GPUs available. However, synchronizing results from each GPU can add overhead.\n",
    "\n",
    "There are two types of data parallelism, DataParallel (DP) and DistributedDataParallel (DDP).\n",
    "\n",
    "**DataParallel**: DataParallel supports distributed training on a single machine with multiple GPUs.\n",
    "\n",
    "- The default GPU, GPU 0, reads a batch of data and sends a mini batch of it to the other GPUs.\n",
    "- An up-to-date model is replicated from GPU 0 to the other GPUs.\n",
    "- A forward pass is performed on each GPU and their outputs are sent to GPU 0 to compute the loss.\n",
    "- The loss is distributed from GPU 0 to the other GPUs for the backward pass.\n",
    "- The gradients from each GPU are sent back to GPU 0 and averaged.\n",
    "\n",
    "**DistributedDataParallel:** DistributedDataParallel supports distributed training across multiple machines with multiple GPUs.\n",
    "\n",
    "- The main process replicates the model from the default GPU, GPU 0, to each GPU.\n",
    "- Each GPU directly processes a mini batch of data.\n",
    "- The local gradients are averaged across all GPUs during the backward pass.\n",
    "- DDP is recommended because it reduces communication overhead between GPUs, efficiently utilizes each GPU, and scales to more than one machine.\n",
    "\n",
    "![DDP](img/ddp.png)\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Model parallelism\n",
    "Model parallelism distributes a model across multiple GPUs. There are several ways to split a model, but the typical method distributes the model layers across GPUs. On the forward pass, the first GPU processes a batch of data and passes it to the next group of layers on the next GPU. For the backward pass, the data is sent backward from the final layer to the first layer.\n",
    "\n",
    "Model parallelism is a useful strategy for training models that are too large to fit into the memory of a single GPU. However, GPU utilization is unbalanced because only one GPU is active at a time. Passing results between GPUs also adds communication overhead and it can be a bottleneck.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. Pipeline parallelism\n",
    "Pipeline parallelism is conceptually very similar to model parallelism, but it‚Äôs more efficient because it reduces the amount of idle GPU time. Instead of waiting for each GPU to finish processing a batch of data, pipeline parallelism creates micro-batches of data. As soon as one micro-batch is finished, it is passed to the next GPU. This way, each GPU can concurrently process part of the data without waiting for the other GPU to completely finish processing a mini batch of data.\n",
    "\n",
    "Pipeline parallelism shares the same advantages as model parallelism, but it optimizes GPU utilization and reduces idle time. But pipeline parallelism can be more complex because models may need to be rewritten as a sequence of nn.Sequential modules and it also isn‚Äôt possible to completely reduce idle time because the last forward pass must also wait for the backward pass to finish.\n",
    "\n",
    "#### 4. Tensor parallelism\n",
    "Tensor parallelism distributes large tensor computations across multiple GPUs. The tensors are sliced horizontally or vertically and each slice is processed by a separate GPU. Each GPU performs its calculations on its tensor slice and the results are synchronized at the end to reconstruct the final result.\n",
    "\n",
    "Tensor parallelism is effective for training large models that don‚Äôt fit into the memory of a single GPU. It is also faster and more efficient because each GPU can process its tensor slice in parallel, and it can be combined with other parallelism methods. Like other parallelism methods though, tensor parallelism adds communication overhead between GPUs.\n",
    "\n",
    "#### 5. Hybrid parallelism\n",
    "Parallelism methods can be combined to achieve even greater memory savings and more efficiently train models with billions of parameters.\n",
    "\n",
    "#### 6. Data parallelism and pipeline parallelism\n",
    "Data and pipeline parallelism distributes the data across GPUs and divides each mini batch of data into micro-batches to achieve pipeline parallelism.\n",
    "Each data parallel rank treats the process as if there were only one GPU instead of two, but GPUs 0 and 1 can offload micro-batches of data to GPUs 2 and 3 and reduce idle time.\n",
    "This approach optimizes parallel data processing by reducing idle GPU utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together: Sentiment Analysis for IMDb reviews\n",
    "\n",
    "#### Step 1: Text Vectorization ‚Äì From Tokens to Tensors\n",
    "\n",
    "Text must be converted into numerical representations for ML models. This process involves:\n",
    "\n",
    "* **Tokenization:** breaking text into units (e.g., words, subwords).\n",
    "* **Embedding:** mapping tokens to vectors.\n",
    "\n",
    "Given a sentence:\n",
    "`\"The cat sat on the mat\"`\n",
    "Tokenize ‚Üí `[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]`\n",
    "\n",
    "Assign integer IDs:\n",
    "\n",
    "$$\n",
    "\\text{Tokens} \\rightarrow \\{12, 89, 44, 32, 12, 78\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c44eacc717a4f2c93924b4b6ee47045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fcaab808ae41fd8e3393e44470b2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdf3d83ae8046cd819bcb76926354b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb5a9fd56464bf087966fe884ea94c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/medhaaga/miniconda3/envs/zsp/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/medhaaga/miniconda3/envs/zsp/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 00:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.001594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/medhaaga/miniconda3/envs/zsp/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=47, training_loss=0.05462673910834054, metrics={'train_runtime': 34.2603, 'train_samples_per_second': 87.565, 'train_steps_per_second': 1.372, 'total_flos': 397402195968000.0, 'train_loss': 0.05462673910834054, 'epoch': 1.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load IMDb (binary classification)\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "train_data = imdb_dataset[\"train\"].shuffle(seed=42).select(range(4000))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "encoded = imdb_dataset.map(tokenize, batched=True)\n",
    "encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Embeddings + Transformer model\n",
    "\n",
    "#### a. **Input Representation**\n",
    "\n",
    "Each token $t_i$ in a sentence is converted to a vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i = E(t_i, i) = \\text{TokenEmbedding}(t_i) + \\text{PositionEmbedding}(i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $t_i$: the $i$-th token (word/piece)\n",
    "* $\\mathbf{x}_i \\in \\mathbb{R}^d$: input embedding vector\n",
    "* $d$: embedding dimension, here 768\n",
    "\n",
    "\n",
    "#### b. **Self-Attention**\n",
    "\n",
    "Each input vector $\\mathbf{x}_i$ is passed through multiple Transformer layers. At each layer, self-attention computes:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\right)\\mathbf{V}\n",
    "$$\n",
    "\n",
    "Where for each token:\n",
    "\n",
    "* $\\mathbf{Q}_i = \\mathbf{W}^Q \\mathbf{x}_i$\n",
    "* $\\mathbf{K}_i = \\mathbf{W}^K \\mathbf{x}_i$\n",
    "* $\\mathbf{V}_i = \\mathbf{W}^V \\mathbf{x}_i$\n",
    "\n",
    "And $\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V$ are learned matrices.\n",
    "\n",
    "The result is a context-aware embedding $\\mathbf{h}_i^{(L)} \\in \\mathbb{R}^d$ after $L$ layers.\n",
    "\n",
    "\n",
    "#### c. **Transformer Layer**:\n",
    "\n",
    "Multiple self-attention heads + feedforward layers:\n",
    "\n",
    "$$\n",
    "\\text{Output}_i = \\text{LayerNorm}(x_i + \\text{FFN}(\\text{SelfAttention}(x)))\n",
    "$$\n",
    "\n",
    "Stacking many such layers, BERT produces final embeddings $\\mathbf{h}_i^{(L)}$ for each word:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_1^{(L)}, \\dots, \\mathbf{h}_n^{(L)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained transformer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define trainer\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded[\"train\"].select(range(3000)),\n",
    "    eval_dataset=encoded[\"train\"].select(range(3000, 4000)),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Stage            | Parallelized Over | Device |\n",
    "| ---------------- | ----------------- | ------ |\n",
    "| Tokenization     | Text samples      | CPU    |\n",
    "| Embedding lookup | Tokens            | GPU    |\n",
    "| Attention scores | Tokens, heads     | GPU    |\n",
    "| Output layers    | Tokens            | GPU    |\n",
    "| Backpropagation  | Parameters        | GPU    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(texts):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        return preds, probs\n",
    "\n",
    "# Try inference\n",
    "sample_texts = [\n",
    "    \"This movie was absolutely wonderful and touching.\",\n",
    "    \"I hated every second of this film. A total waste of time.\"\n",
    "]\n",
    "\n",
    "preds, probs = predict_sentiment(sample_texts)\n",
    "\n",
    "for text, p, pr in zip(sample_texts, preds, probs):\n",
    "    sentiment = \"Positive\" if p == 1 else \"Negative\"\n",
    "    print(f\"\\nText: {text}\\nPrediction: {sentiment} (confidence: {pr[p]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.16 ('zsp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee572519086218b94d40792eb4df8d070dce6b9c8f3771ec28722a77608dce3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

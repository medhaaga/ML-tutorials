{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT 548/CSE 547 Tutorial: Parallel Processing\n",
    "\n",
    "### **Instructor**:</b> Marina Meila\n",
    "### **TA**: Medha Agarwal\n",
    "### **Date**: May 09, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è∞ Previously...\n",
    "\n",
    "We looked at vectorization - both as a process of converting scalar operations (loop-based) into array operations and as a method for converting input data from its raw format (i.e. text ) into vectors of real numbers.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò Tutorial Overview\n",
    "\n",
    "In this tutorial, we will look at methods for paralell processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Compute Environments\n",
    "\n",
    "\n",
    "### 1.1. Basic Hardware\n",
    "\n",
    "*Source:* [NVIDIA](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/) and [TDS](https://towardsdatascience.com/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d)\n",
    "\n",
    "- Processors are electronic circuitry that execute computer instructions, whose minimal unit is a **transistor**. \n",
    "- The two standard processors on most systems are called the **Central Processing Unit (CPU)** and the **Graphic Processing Unit (GPU)**.\n",
    "- These processors differ in their allotment of transistors to computing processes. \n",
    "    - CPUs dedicate these transistors into a small number of **logical cores**, with a stronger emphasis on circuitry used for **caches** (data storage that speeds up access) and **control flow** (`if-else` statements, `for` and `while` loops, etc). \n",
    "    - GPUs, on the other hand, can have thousands of cores, each capable only of simple arithmetic operations.\n",
    "- Thus, for highly parallelizable operations (those that can be split into independent **threads**), such as matrix multiplications common in ML/DL, GPUs have become an essential part of any computing infrastructure.\n",
    "- That being said, parallelizing more complex operations, such as hyperparameter search, is still done on machines with a large number of CPU cores. Examples include **CPU Clusters** such as the Stat cluster, which uses a manager called Slurm to schedule computation.\n",
    "- **Tensor Processing Units (TPUs)** are processors that are built specifically for the tensor operations in neural networks training. Specifically, it was made to speed up the operations of TensorFlow, which used to be the dominant DL framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `htop` in a terminal in order to \"see\" the physical CPU cores at work. When running operations that make use of all of the cores (such as a large matrix multiplications), you should see the bars light up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "n_sim = 10000\n",
    "n = 10000\n",
    "\n",
    "try:\n",
    "    for i in range(10000):\n",
    "        A = np.random.normal(size=(n, n))\n",
    "        B = np.random.normal(size=(n, n))\n",
    "\n",
    "        C = A @ B\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Graceful Exit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working on a server, use `tmux` to let processes run in the background while you can break the connection. This is not only a convenient method, but very safe as well. A few relevant commands:\n",
    "- `tmux`: Create a session.\n",
    "- `ctrl + B`, then `%`: Create a panel within a session.\n",
    "- `ctrl + B`, then `D`: Detach from a session.\n",
    "- `tmux ls`: List the current sessions.\n",
    "- `tmux attach -t <session_name>`: Attach to a running session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parallelism over CPU and GPU\n",
    "\n",
    "Parallel programming is a method of speeding up computations by dividing tasks across multiple processing units that execute simultaneously. In machine learning, this is essential for handling large datasets and complex models efficiently. \n",
    "\n",
    "- CPU design philosophy: optimized for sequential processing\n",
    "\n",
    "  - Fewer, more powerful cores with complex control units\n",
    "  - Large caches and branch prediction\n",
    "  - Optimized for irregular memory access and control flow\n",
    "  - Parallelism achieved through multithreading or multiprocessing - where a few powerful cores execute different parts of a task concurrently‚Äîuseful for data loading or feature processing.\n",
    "\n",
    "- GPU design philosophy: optimized for parallel processing\n",
    "  - Thousands of simpler cores\n",
    "  - Specialized for floating-point operations\n",
    "  - SIMT (Single Instruction, Multiple Thread) execution model\n",
    "  - Memory architecture optimized for throughput\n",
    "  - well-suited for operations like matrix multiplication and deep learning training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Application 1: Vectorized Implementation\n",
    "\n",
    "**Implicit Parallelism**\n",
    "\n",
    "- Because matrix operations are the prototypical use case of parallelism, many computing processes can benefit from parallelization implicitly by being represented in linear algebraic form.\n",
    "- Such implementations are said to be **vectorized**. The main idea is to avoid all `for` loops where possible, and use libraries like `numpy` and `pytorch` for matrix and tensor algebra.\n",
    "- To develop software/algorithms in any kind of large-scale or production setting, it is very important to make vectorization a constant habit. A core part of why GPUs (and even CPUs) make processes faster is by distributing the load across all of the logical cores. Using `for` loops ruins this benefit and wastes valuable compute time.\n",
    "- Another separate but related issue is list concatenation; if you are trying to apply a function to a sequence of outputs, it is sometimes tempting to have a \"running\" list for which you concatenate the new value through the iteration of a `for` loop. In these sitatuations, it is always better to **pre-allocate** arrays, or be confident that you are **appending** (in $O(1)$) instead of copying the entire contents of the array every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Nadaraya-Watson with Gaussian Kernel\n",
    "\n",
    "In this example, we implement multiple versions of the Nadaraya-Watson regression estimator. Specifically, consider a supervised learning problem in which we are given an $n$-sized training set $(x_1, y_1), ..., (x_n, y_n)$ of feature vectors $x_i \\in \\mathbb{R}^d$ and label $y_i \\in \\mathbb{R}$. Given a test point $x \\in \\mathbb{R}^d$, the predicted value of the label $\\hat{y}$ is given by\n",
    "\\begin{equation}\n",
    "    \\hat{y} = \\sum_{i=1}^n \\frac{y_i k\\left(\\frac{x - x_i}{h}\\right)}{\\sum_{j=1}^n k\\left(\\frac{x - x_j}{h}\\right)},\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{align*}\n",
    "    k(z) = e^{-\\frac{1}{2}||z||^2}\n",
    "\\end{align*}\n",
    "is the Gaussian kernel and and $h > 0$ is a bandwidth parameter. Kernel methods offer plenty of opportunities to replace wasteful `for` loops with efficient vectorized implementations, especially when $n$ and $d$ are very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_gaussian_kernel(x1, x2, h):\n",
    "    sq_norm = 0\n",
    "    for j in range(len(x1)):\n",
    "        sq_norm += (x1[j] - x2[j]) ** 2\n",
    "    return np.exp(-0.5 * sq_norm / h ** 2)\n",
    "\n",
    "class NaiveNadarayaWatsonRegressor:\n",
    "    def __init__(self, bandwidth, kernel=naive_gaussian_kernel):\n",
    "        self.h = bandwidth\n",
    "        self.kernel = kernel\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = check_array(X)\n",
    "        \n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            num = 0\n",
    "            denom = 0\n",
    "            for i, x_i in enumerate(self.X_train):\n",
    "                num += self.y_train[i] * self.kernel(x, x_i, self.h)\n",
    "                denom += self.kernel(x, x_i, self.h)\n",
    "            y_pred.append(num / denom)\n",
    "            \n",
    "        return np.array(y_pred).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Why does the `fit` method above simply store the training data? \n",
    "\n",
    "**Exercise 2:** What are other algorithms that might be implemented in this way? (These are referred to as **lazy learners**.\n",
    "\n",
    "**Exercise 3:** Based on our discussion of nearest neighbors in high dimensions, what kinds of operations could occur in the `fit` method for approximate lazy learners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, d):\n",
    "    X = np.random.normal(size=(n, d))\n",
    "    coef = np.random.normal(size=(d,))\n",
    "    intercept = np.random.normal()\n",
    "    noise = np.random.normal(size=(n,))\n",
    "    y = (X @ coef + intercept + noise).reshape(-1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a test set $(x_1, y_1), ..., (x_n, y_n)$, we measure performance using the $R^2$ score (also known as explained variance).\n",
    "\\begin{equation}\n",
    "    R^2 = \\frac{\\sum_{i=1}^n (\\hat{y}_i - y_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2},\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "    \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "\\end{equation}\n",
    "is the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 10)\n",
      "(8000,)\n",
      "(2000, 10)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "# DEMO: \n",
    "np.random.seed(123)\n",
    "\n",
    "n = 10000\n",
    "d = 10\n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_data(n, d)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too slow! :'(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    start = time.time()\n",
    "    naive_model = NaiveNadarayaWatsonRegressor(bandwidth=0.5).fit(X_train, y_train)\n",
    "    y_pred = naive_model.predict(X_test)\n",
    "    print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "    end = time.time()\n",
    "    print(\"Time: %0.4f seconds.\" % (end - start))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Too slow! :'(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give evalutation set $z_1, ..., z_m$, let $K \\in \\mathbb{R}^{m \\times n}$ be the matrix such that \n",
    "\\begin{align*}\n",
    "    K_{ij} = k\\left(\\frac{z_i - x_j}{h}\\right).\n",
    "\\end{align*}\n",
    "Then, let $y = (y_1, ..., y_n)^\\top \\in \\mathbb{R}^n$ and $\\hat{y} = (\\hat{y}_1, ..., \\hat{y}_m)^\\top \\in \\mathbb{R}^m$. It holds that \n",
    "\\begin{align*}\n",
    "\\hat{y} = \\frac{Ky}{K 1_n},\n",
    "\\end{align*}\n",
    "where the division is element-wise. This will lead to a vectorized implementation. For now, we use the `sklearn` function for `pairwise_distances`, but we will soon see how to make a vectorized implementation of this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedNadarayaWatsonRegressor:\n",
    "    def __init__(self, bandwidth):\n",
    "        self.h = bandwidth\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = check_array(X)\n",
    "        \n",
    "        # n_eval by n_train kernel matrix.\n",
    "        kernels = np.exp(-0.5 * pairwise_distances(X, self.X_train) ** 2 / self.h ** 2)\n",
    "        \n",
    "        # Element-wise division\n",
    "        return (kernels @ self.y_train) / kernels.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** How can we use matrix operations to implement a vectorized version of pairwise Euclidean distances? That is, given the matrix $X \\in \\mathbb{R}^{n_1 \\times d}$, and the matrix $Z \\in \\mathbb{R}^{n_1 \\times d}$, how can we produce the matrix $D \\in \\mathbb{R}^{n_1 \\times n_2}$, where\n",
    "\\begin{align*}\n",
    "    D_{ij} = ||x_i - z_j||_2 ?\n",
    "\\end{align*}\n",
    "\n",
    "**Solution:** Represent the Euclidean distance as a scalar product.\n",
    "\\begin{align*}\n",
    "    D_{ij} = ||x_i - z_j||_2 = \\sqrt{(x_i - z_j)^\\top (x_i - z_j)} = \\sqrt{x_i^\\top x_i + z_j^\\top z_j - 2 x_i^\\top z_j}.\n",
    "\\end{align*}\n",
    "The $x_i^\\top x_i$ and $z_j^\\top z_j$ terms can be computed by taking the norms of the rows of $X$ and $Z$. The cross term $x_i^\\top z_j$ can be computed via the matrix multiplication $X^\\top Z$. Do we need to compute $X^\\top X$ or $Z^\\top Z$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.7880181382483237\n",
      "Time: 0.2995 seconds.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    start = time.time()\n",
    "    vectorized_model = VectorizedNadarayaWatsonRegressor(bandwidth=0.5).fit(X_train, y_train)\n",
    "    y_pred = vectorized_model.predict(X_test)\n",
    "    print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "    end = time.time()\n",
    "    print(\"Time: %0.4f seconds.\" % (end - start))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Too slow! :'(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending to the GPU\n",
    "\n",
    "- These kinds of operations can be made even faster by using a GPU. In order to interact with it, we use the `torch` package (PyTorch).\n",
    "- Using PyTorch is largely the same as `numpy`. The main difference is that the quintessential array object becomes a `torch.tensor` instead of a `numpy.ndarray`. Second, tensors exist on a \"device\" which can either be `\"cpu\"` for the CPU, or `\"cuda\"` for the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def set_device():\n",
    "    # If there's a GPU available...\n",
    "    if torch.cuda.is_available():\n",
    "\n",
    "        # Tell PyTorch to use the GPU.\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "        print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    # If not...\n",
    "    else:\n",
    "        print(\"No GPU available, using the CPU instead.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "def pdist(sample_1, sample_2, eps=1e-5):\n",
    "    n_1, n_2 = sample_1.size(0), sample_2.size(0)\n",
    "    norms_1 = torch.sum(sample_1**2, dim=1, keepdim=True)\n",
    "    norms_2 = torch.sum(sample_2**2, dim=1, keepdim=True)\n",
    "    norms = (norms_1.expand(n_1, n_2) +\n",
    "             norms_2.transpose(0, 1).expand(n_1, n_2))\n",
    "    distances_squared = norms - 2 * sample_1.mm(sample_2.t())\n",
    "    return torch.sqrt(eps + torch.abs(distances_squared))\n",
    "\n",
    "class GPUNadarayaWatsonRegressor:\n",
    "    def __init__(self, bandwidth):\n",
    "        self.h = bandwidth\n",
    "        self.device = set_device()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        self.X_train = torch.from_numpy(X).to(self.device)\n",
    "        self.y_train = torch.from_numpy(y).to(self.device)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = check_array(X)\n",
    "        X = torch.from_numpy(X).to(self.device)\n",
    "        \n",
    "        # n_eval by n_train kernel matrix.\n",
    "        kernels = torch.exp(-0.5 * pdist(X, self.X_train) ** 2 / self.h ** 2)\n",
    "        \n",
    "        # Element-wise division\n",
    "        return (torch.matmul(kernels, self.y_train) / kernels.sum(axis=1)).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n",
      "R^2:  0.7880181382483237\n",
      "Time: 0.3924 seconds.\n"
     ]
    }
   ],
   "source": [
    "# DEMO: Run in Colab environment.\n",
    "try:\n",
    "    start = time.time()\n",
    "    gpu_model = GPUNadarayaWatsonRegressor(bandwidth=0.5).fit(X_train, y_train)\n",
    "    y_pred = gpu_model.predict(X_test)\n",
    "    print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "    end = time.time()\n",
    "    print(\"Time: %0.4f seconds.\" % (end - start))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Too slow! :'(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Application 2: Embarassingly Parallel Loops\n",
    "\n",
    "**Definition**\n",
    "\n",
    "- In the previous section we were using a combination of linear algebra representations and built-in functions to benefit from parallelization. This method only works for inherently mathematical objects, which is also the reason we got a speed up from the GPU.\n",
    "- When we would like to parallelize more complicated programming logic, we have to use explicit parallelization. This means we describe explicitly a \"batch\" operation which should be split among various threads.\n",
    "- A special (but very common) case of this situation is **embarrassingly parallel loops**. This occurs when there is a `for` loop for which the result of computation in each iteration has no relationship to the result of computation in other iterations. \n",
    "- In these cases, the code essentially \"looks\" like a `for` loop. We just have to instruct the machine to split this load over a certain number of cores.\n",
    "- One thing to note that explicit parallelism often has an overhead cost, so sometimes for small scale problems it can even be slower than just iterating sequentially. Doing this correctly requires some knowledge of the hardware system its running on.\n",
    "\n",
    "**Exercise 4.1:** Please name some iterated computations in machine learning settings that are and are not embarrassingly parallelizable. \n",
    "\n",
    "**Exercise 4.2:** Based on their description, are embarrassingly parallel workloads better distributed over CPU cores or GPU cores? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 4.1**\n",
    "\n",
    "*Embarrassingly parallelizable computations:*\n",
    "\n",
    "* **Inference on independent samples**: Making predictions on multiple test inputs where each input can be evaluated separately.\n",
    "* **Hyperparameter tuning** (e.g., grid search or random search): Each model configuration can be trained independently.\n",
    "* **Cross-validation folds**: Training models on different folds does not depend on the others.\n",
    "* **Data augmentation (offline)**: Applying random transformations to different images or sequences independently.\n",
    "\n",
    "*Not embarrassingly parallelizable computations:*\n",
    "\n",
    "* **Gradient descent with shared parameters**: Each iteration depends on the model state updated in previous steps.\n",
    "* **Sequential RNN training**: Later steps depend on hidden states from earlier ones.\n",
    "* **Backpropagation through time (BPTT)** in recurrent models: Requires passing gradients backward through dependent steps.\n",
    "* **Batch normalization during training**: Requires computing statistics (mean/variance) over the entire batch, introducing dependencies.\n",
    "\n",
    "**Exercise 4.2**\n",
    "\n",
    "*Embarrassingly parallel workloads are often better distributed over GPU cores**‚Äî**but it depends on the nature of the task*.\n",
    "\n",
    "* **GPUs** are ideal when the computation per task is numeric-heavy, uniform, and can be written in a vectorized or SIMD fashion. They offer massive throughput for simple operations on large datasets (e.g., image processing, vector math, matrix ops).\n",
    "* **CPUs** are often better when each task involves complex logic, branching, or irregular memory access, which GPUs handle less efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to implementation, typical options in Python are `joblib` and `multiprocessing` for parallel CPU processing and PyTorch with CUDA for GPU-based inference and augmentation. In R, the `apply` family is a common option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Inference on independent samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CPU parallelism (e.g., with scikit-learn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (10000, 784)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000, n_jobs=-1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, n_jobs=-1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST (~70,000 samples, 784 features)\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Reduce size for faster training, but keep test large for benchmarking\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:20000], y[:20000], test_size=0.5, random_state=42)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000, solver='lbfgs', n_jobs=-1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial inference time: 0.3704 seconds\n",
      "Parallel inference time: 1.2726 seconds\n"
     ]
    }
   ],
   "source": [
    "start_serial = time.time()\n",
    "\n",
    "y_pred_serial = [model.predict(x.reshape(1, -1))[0] for x in X_test]\n",
    "\n",
    "end_serial = time.time()\n",
    "print(f\"Serial inference time: {end_serial - start_serial:.4f} seconds\")\n",
    "\n",
    "def predict_one(x):\n",
    "    return model.predict(x.reshape(1, -1))[0]\n",
    "\n",
    "start_parallel = time.time()\n",
    "\n",
    "y_pred_parallel = Parallel(n_jobs=-1)(delayed(predict_one)(x) for x in X_test)\n",
    "\n",
    "end_parallel = time.time()\n",
    "print(f\"Parallel inference time: {end_parallel - start_parallel:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using a GPU-compatible model (e.g., PyTorch), batch inference uses GPU cores naturally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n",
      "GPU inference time: 0.6741 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST and move to GPU\n",
    "transform = transforms.ToTensor()\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "print(test_dataset.data.shape)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "# Simple model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize and move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleNN().to(device)\n",
    "\n",
    "# Dummy training step (skipped here) ‚Äì just use random weights\n",
    "model.eval()\n",
    "\n",
    "# Inference on GPU\n",
    "start_gpu = time.time()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.append(preds.cpu())\n",
    "\n",
    "end_gpu = time.time()\n",
    "print(f\"GPU inference time: {end_gpu - start_gpu:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hyperparameter tuning\n",
    "\n",
    "We use `joblib` to search for the best bandwidth for the Nadaraya-Watson estimator. As before, use `htop` to ensure that your code is working properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/55/fdrvzw6533l1ygg771zsj0y80000gn/T/ipykernel_34877/1701722876.py:20: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/medha/miniconda3/envs/wildlife/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/Users/medha/miniconda3/envs/wildlife/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/medha/miniconda3/envs/wildlife/lib/python3.11/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/medha/miniconda3/envs/wildlife/lib/python3.11/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/55/fdrvzw6533l1ygg771zsj0y80000gn/T/ipykernel_34877/758671094.py\", line 8, in worker\n  File \"/Users/medha/miniconda3/envs/wildlife/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/medha/miniconda3/envs/wildlife/lib/python3.11/site-packages/sklearn/metrics/_regression.py\", line 1204, in r2_score\n    _, y_true, y_pred, multioutput = _check_reg_targets(\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/medha/miniconda3/envs/wildlife/lib/python3.11/site-packages/sklearn/metrics/_regression.py\", line 113, in _check_reg_targets\n    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/medha/miniconda3/envs/wildlife/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n    _assert_all_finite(\n  File \"/Users/medha/miniconda3/envs/wildlife/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/Users/medha/miniconda3/envs/wildlife/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input contains NaN.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_parallel:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# Use n_jobs to determine how many cores will be used. Negative values imply \"all but\".\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m         r2 \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)(delayed(worker)(bandwidth) \u001b[38;5;28;01mfor\u001b[39;00m bandwidth \u001b[38;5;129;01min\u001b[39;00m bandwidths)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m         r2 \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/wildlife/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[39m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/wildlife/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wildlife/lib/python3.11/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[39m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[39m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[39m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[39m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_error_fast()\n\u001b[1;32m   1755\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[39m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wildlife/lib/python3.11/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[39m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[39m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[39m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[39mif\u001b[39;00m error_job \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     error_job\u001b[39m.\u001b[39mget_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/wildlife/lib/python3.11/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[39m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[39m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[39m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_or_raise()\n\u001b[1;32m    747\u001b[0m \u001b[39m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/wildlife/lib/python3.11/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "is_parallel = True\n",
    "\n",
    "bandwidths = np.logspace(-1, 3, 30)\n",
    "\n",
    "# Define function that performs each iteration.\n",
    "def worker(bandwidth):\n",
    "    model = VectorizedNadarayaWatsonRegressor(bandwidth=bandwidth).fit(X_train, y_train)\n",
    "    return r2_score(y_test, model.predict(X_test))\n",
    "\n",
    "try:\n",
    "    if is_parallel:\n",
    "        # Use n_jobs to determine how many cores will be used. Negative values imply \"all but\".\n",
    "        r2 = Parallel(n_jobs=-2)(delayed(worker)(bandwidth) for bandwidth in bandwidths)\n",
    "    else:\n",
    "        r2 = []\n",
    "        for bandwidth in bandwidths:\n",
    "            r2.append(worker(bandwidth))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Too slow! :'('\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bandwidths, r2, linewidth=3, color=\"red\")\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Bandwidth\")\n",
    "plt.ylabel(r\"$R^2$ Score\")\n",
    "plt.title(\"Nadaraya-Watson Performance vs. Bandwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Cross-validation folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Data augmentation (offline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.10 ('wildlife')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e91bf06cdda86781ed42e4dcd29b8f1b8cce196a91f497f0a418c21906987625"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
